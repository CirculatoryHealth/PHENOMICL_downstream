{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing SumStats for GWAS Catalog\n",
    "\n",
    "This notebook serves to inspect `regenie output of GWASToolKit`, and proceses the GWAS data for sharing through *GWAS Catalog*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gwaslab==3.4.44\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install -c conda-forge polars"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check for installation of required packages\n",
    "def check_install_package(package_name):\n",
    "    try:\n",
    "        importlib.import_module(package_name)\n",
    "    except ImportError:\n",
    "        print(f'{package_name} is not installed. Installing it now...')\n",
    "        subprocess.check_call(['pip', 'install', package_name])\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# argument parsing\n",
    "import argparse\n",
    "\n",
    "# get date and time\n",
    "from datetime import datetime\n",
    "\n",
    "# Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool\n",
    "check_install_package('pandas')\n",
    "import pandas as pd\n",
    "\n",
    "# pyarrow is supperior to loading parquet files\n",
    "check_install_package('pyarrow')\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# polars is a fast dataframe library\n",
    "#check_install_package('polars')\n",
    "#import polars as pl\n",
    "\n",
    "# for statistical analysis\n",
    "check_install_package('scipy')\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "# scientific colourmaps\n",
    "# https://www.fabiocrameri.ch/ws/media-library/8c4b111121ff448e843dfef9220bf613/readme_scientificcolourmaps.pdf\n",
    "check_install_package('cmcrameri')\n",
    "import cmcrameri as ccm\n",
    "from cmcrameri import cm\n",
    "\n",
    "# for plotting\n",
    "check_install_package('matplotlib')\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# if using a Jupyter notebook, include:\n",
    "%matplotlib inline\n",
    "\n",
    "# use Seaborn for visualisations\n",
    "check_install_package('seaborn')\n",
    "import seaborn as sns\n",
    "\n",
    "# for handling GWAS data\n",
    "import gwaslab as gl\n",
    "\n",
    "from sklearn.metrics import jaccard_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating directories for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "from subprocess import check_output\n",
    "\n",
    "# Set general defaults\n",
    "POPULATION = \"EUR\"  # Population group\n",
    "PHENOTYPE = \"PHENOTYPE_NAME\"  # Placeholder for phenotype name\n",
    "REF_1KG = \"1kg_eur_hg38\"  # Reference data\n",
    "\n",
    "# General plotting directory\n",
    "PLOTS_loc = f\"PLOTS/{PHENOTYPE}\"\n",
    "\n",
    "# Check if the general plotting directory exists, if not, create it\n",
    "if not os.path.exists(PLOTS_loc):\n",
    "    os.makedirs(PLOTS_loc)\n",
    "\n",
    "# Regional association plots directory\n",
    "REG_PLOTS_loc = os.path.join(PLOTS_loc, \"Regional_Association_Plots\")\n",
    "\n",
    "# Check if the regional association plots directory exists, if not, create it\n",
    "if not os.path.exists(REG_PLOTS_loc):\n",
    "    os.makedirs(REG_PLOTS_loc)\n",
    "\n",
    "# GWAS results directory - CHANGE to your specific path\n",
    "GWAS_RES_loc = \"/path/to/gwas/results\"\n",
    "print(\"Checking contents of the GWAS results directory:\")\n",
    "print(check_output([\"ls\", os.path.join(GWAS_RES_loc)]).decode(\"utf8\"))\n",
    "\n",
    "# GWAS datasets directory - CHANGE to your specific path\n",
    "GWAS_DATASETS_loc = \"/path/to/gwas/datasets\"\n",
    "print(\"Checking contents of the GWAS datasets directory:\")\n",
    "print(check_output([\"ls\", os.path.join(GWAS_DATASETS_loc)]).decode(\"utf8\"))\n",
    "\n",
    "# Check if the GWAS Catalog directory exists, if not, create it\n",
    "if not os.path.exists(\"GWASCatalog\"):\n",
    "    os.makedirs(\"GWASCatalog\")\n",
    "\n",
    "# GWAS Catalog directory\n",
    "GWASCatalog_loc = os.path.join(\"GWASCatalog\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "Loading the different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Input data files are available in the \"../input/\" directory.\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "# from subprocess import check_output\n",
    "\n",
    "# print(check_output(\n",
    "#     [\"ls\", os.path.join(GWAS_RES_loc, SUBSTUDY_PHENO)]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# carotid intima media thickness\n",
    "\n",
    "We load in the GWAS on cIMT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving cIMT\n",
    "\n",
    "Here we save the parsed data for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data\n",
    "# https://stackoverflow.com/questions/33813815/how-to-read-a-parquet-file-into-pandas-dataframe\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "temp = pl.read_csv(\n",
    "    source=os.path.join(\n",
    "        GWAS_DATASETS_loc\n",
    "        # + \"/AEGS.GWAS.1kGp3v5GoNL5.CD68_counts_rankNorm.EXCL_DEFAULT.summary_results.txt.gz\", #CHANGE!\n",
    "        + \"/regenie.IPH.summary_results.txt.gz\", #CHANGE!\n",
    "    ),\n",
    "    has_header=True,\n",
    "    separator=\" \",\n",
    "    ignore_errors=False,\n",
    "    # n_rows=50000, # for debugging\n",
    "    quote_char=None,\n",
    "    # necessary to fix issues with missing values when reading data\n",
    "    null_values=[\"NA\"],\n",
    "    # There is an error at import (from temp to pandas()):\n",
    "    # Could not parse `X` as dtype `i64` at column 'CHR' (column number 2)\n",
    "    # https://stackoverflow.com/questions/75797640/how-to-specify-column-types-in-python-polars-read-csv\n",
    "    # https://stackoverflow.com/questions/71790235/switching-between-dtypes-within-a-dataframe\n",
    "    # https://pola-rs.github.io/polars/user-guide/concepts/data-types/\n",
    "    dtypes={\"CHR\": pl.Utf8},\n",
    ")\n",
    "# change polars dataframe to pandas dataframe\n",
    "cd_norm_data = temp.to_pandas()\n",
    "del temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspection of data\n",
    "\n",
    "Here we do a quick check on what is what in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heads\n",
    "\n",
    "Printing head of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapes\n",
    "\n",
    "Printing shapes of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Printing shape of data:\\n\", cd_norm_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns\n",
    "\n",
    "Printing columns of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Printing columns of data:\\n\", cd_norm_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Info\n",
    "\n",
    "Printing info of Women-only data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Printing info of data:\\n\", cd_norm_data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column statistics\n",
    "\n",
    "Getting some per column summary statistics of Women-only data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Printing describe of data:\\n\", cd_norm_data.describe())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic visualisations\n",
    "\n",
    "Here we plot histograms of allele frequencies, effect, and sample size ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampling from the data\n",
    "Here we provide an example on how to take a sample of n=800,000 rows, representing Â±10% of the data, for easy plotting. This should be representative for most things we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example code to get a sample\n",
    "\n",
    "# cd_norm_data_sample = cd_norm_data.sample(800000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.histplot(\n",
    "    data=cd_norm_data,  # gwas_combo_sample,\n",
    "    x=\"A1FREQ\",\n",
    "    bins=25,\n",
    "    kde=False,\n",
    "    stat=\"frequency\",\n",
    "    color=\"#1290D9\",\n",
    ")\n",
    "\n",
    "# plt.savefig(\n",
    "#     os.path.join(PLOTS_loc, \"histogram.EAF.\" + POPULATION + \".png\"),\n",
    "#     dpi=300,\n",
    "#     bbox_inches=\"tight\",\n",
    "#     format=\"png\",\n",
    "# )\n",
    "plt.savefig(\n",
    "    os.path.join(PLOTS_loc, \"histogram.EAF.\" + PHENOTYPE + \"_\" + POPULATION + \".pdf\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    "    format=\"pdf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(\n",
    "    data=cd_norm_data,  # gwas_combo_sample,\n",
    "    # x=\"BETA_FIXED\",\n",
    "    x=\"BETA\",\n",
    "    bins=25,\n",
    "    kde=False,\n",
    "    stat=\"frequency\",\n",
    "    color=\"#E55738\",\n",
    ")\n",
    "\n",
    "# plt.savefig(\n",
    "#     os.path.join(PLOTS_loc, \"histogram.effect.\" + POPULATION + \".png\"),\n",
    "#     dpi=300,\n",
    "#     bbox_inches=\"tight\",\n",
    "#     format=\"png\",\n",
    "# )\n",
    "plt.savefig(\n",
    "    os.path.join(PLOTS_loc, \"histogram.effect.\" + PHENOTYPE + \"_\" + POPULATION + \".pdf\"),\n",
    "    dpi=300,\n",
    "    bbox_inches=\"tight\",\n",
    "    format=\"pdf\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SumStats quality control\n",
    "\n",
    "Here we first check and fix headers, and contents of the SumStats object.\n",
    "\n",
    "First, we load the data and inspect it using `GWASLab`. Note that for this to work, some column types need to be adjusted.\n",
    "\n",
    "```\n",
    " 0   SNP                object \n",
    " 1   CHR                object \n",
    " 2   POS                int64  \n",
    " 3   HAPMAP_A1_FREQ     float64\n",
    " 4   CODED_ALLELE       object \n",
    " 5   NONCODED_ALLELE    object \n",
    " 6   CODED_ALLELE_FREQ  float64\n",
    " 7   N_EFF              float64\n",
    " 8   P_SQRTN            float64\n",
    " 9   BETA_FIXED         float64\n",
    " 10  SE_FIXED           float64\n",
    " 11  P_FIXED            float64\n",
    " 12  P_RANDOM           float64\n",
    " 13  DF                 int64  \n",
    " 14  P_COCHRANS_Q       float64\n",
    " 15  I_SQUARED          float64\n",
    " 16  DIRECTIONS         object \n",
    " 17  GENES_1000KB       object \n",
    " 18  NEAREST_GENE       object \n",
    " 19  CAVEAT             object \n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data['P'] = 10**(-cd_norm_data.LOG10P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data[[\"CHROM\"]] = cd_norm_data[[\"CHROM\"]].astype(\"object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data[[\"POS\"]] = cd_norm_data[[\"GENPOS\"]].astype(\"Int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd_norm_data['CAVEAT'].fillna('None', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create new SNPID column based on chromosome, position, and alleles\n",
    "# # down the road we need an SNPID column to merge with the reference data and which does not contain 'ID' because this is not correctly interpreted by GWASLab\n",
    "cd_norm_data[\"SNPID\"] = (\n",
    "    cd_norm_data[\"CHROM\"].astype(str)\n",
    "    + \":\"\n",
    "    + cd_norm_data[\"POS\"].astype(str)\n",
    "    + \":\"\n",
    "    + cd_norm_data[\"ALLELE0\"].astype(str)\n",
    "    + \":\"\n",
    "    + cd_norm_data[\"ALLELE1\"].astype(str)\n",
    ")\n",
    "\n",
    "# # from an *.out file at /hpc/dhl_ec/svanderlaan/projects/sign/vonberg_joanna/2.Age.of.onset/linreg_aoo_unrestricted_bolt/amr.everything.butwhi/xx.group\n",
    "# # cd_norm_data[\"N\"] = 5344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data.rename(columns={\"SNPID\": \"VariantID\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to GWASLab object\n",
    "\n",
    "Here we are ready to load the data and inspect it using `GWASLab`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gwaslab as gl\n",
    "\n",
    "# Specify the columns: - CHANGE!\n",
    "cd_norm_data_sumstats = gl.Sumstats(\n",
    "    cd_norm_data,\n",
    "    snpid=\"VariantID\",\n",
    "    # rsid=\"RSID\", # not available\n",
    "    chrom=\"CHROM\",\n",
    "    pos=\"POS\",\n",
    "    ea=\"ALLELE1\",\n",
    "    nea=\"ALLELE0\",\n",
    "    eaf=\"A1FREQ\",\n",
    "    # maf=\"MAF\",\n",
    "    beta=\"BETA\",\n",
    "    se=\"SE\",\n",
    "    p=\"P\",\n",
    "    # direction=\"Direction\",  # only for meta-GWAS\n",
    "    n=\"N\",\n",
    "    # info=\"Info\", # not available\n",
    "    other=[\n",
    "        # \"MAC\",\n",
    "        # \"MAF\"\n",
    "    #     \"DF\",\n",
    "    #     \"DIRECTIONS\",\n",
    "    #     \"P_COCHRANS_Q\",\n",
    "    #     \"I_SQUARED\",\n",
    "    #     \"CAVEAT\",\n",
    "    ],\n",
    "    build=\"38\",\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data_sumstats.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data_sumstats.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data_sumstats.lookup_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intermediate cleaning\n",
    "\n",
    "Here we cleanup the originally loaded data, to clear memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cd_norm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get reference data\n",
    "\n",
    "We align the data to the reference genome, this will work for most common variants. Before that, we check which reference datasets are available, and get these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check references\n",
    "gl.check_available_ref()\n",
    "\n",
    "# {'1kg_eas_hg19': 'https://www.dropbox.com/s/lztaxqhy2o6dpxw/EAS.ALL.split_norm_af.1kgp3v5.hg19.vcf.gz?dl=1',\n",
    "#  '1kg_eas_hg19_md5': 'c8c97434843c0da3113fc06879ead472',\n",
    "#  '1kg_eas_hg19_tbi': 'https://www.dropbox.com/s/k9klefl8m9fcfo1/EAS.ALL.split_norm_af.1kgp3v5.hg19.vcf.gz.tbi?dl=1',\n",
    "#  '1kg_eur_hg19': 'https://www.dropbox.com/s/1nbgqshknevseks/amr.ALL.split_norm_af.1kgp3v5.hg19.vcf.gz?dl=1',\n",
    "#  '1kg_eur_hg19_md5': '734069d895009d38c2f962bfbb6fab52',\n",
    "#  '1kg_eur_hg19_tbi': 'https://www.dropbox.com/s/vscvkrflh6fc5a0/amr.ALL.split_norm_af.1kgp3v5.hg19.vcf.gz.tbi?dl=1',\n",
    "#  '1kg_eas_hg38': 'https://www.dropbox.com/s/3dstbbb1el9r3au/EAS.ALL.split_norm_af.1kg_30x.hg38.vcf.gz?dl=1',\n",
    "#  '1kg_eas_hg38_md5': 'f45e80bca9ef7b29e6b1832e6ac15375',\n",
    "#  '1kg_eas_hg38_tbi': 'https://www.dropbox.com/s/vwnp5vd8dcqksn4/EAS.ALL.split_norm_af.1kg_30x.hg38.vcf.gz.tbi?dl=1',\n",
    "#  '1kg_eur_hg38': 'https://www.dropbox.com/s/z0mkehg17lryapv/amr.ALL.split_norm_af.1kg_30x.hg38.vcf.gz?dl=1',\n",
    "#  '1kg_eur_hg38_md5': '228d3285fa99132cc6321e2925e0768d',\n",
    "#  '1kg_eur_hg38_tbi': 'https://www.dropbox.com/s/ze8g58x75x9qbf0/amr.ALL.split_norm_af.1kg_30x.hg38.vcf.gz.tbi?dl=1',\n",
    "#  '1kg_sas_hg19': 'https://www.dropbox.com/scl/fi/fubqvuj3p4ii4y35zknv8/SAS.ALL.split_norm_af.1kgp3v5.hg19.vcf.gz?rlkey=5z50f66iltjchcaszznq5bczt&dl=1',\n",
    "#  '1kg_sas_hg19_md5': 'e2d3f9e2e6580d05e877e9effd435c4e',\n",
    "#  '1kg_sas_hg19_tbi': 'https://www.dropbox.com/scl/fi/icnmrnzee7ofdpx5l96tg/SAS.ALL.split_norm_af.1kgp3v5.hg19.vcf.gz.tbi?rlkey=st8t88snby26q37rqi6zh5zck&dl=1',\n",
    "#  '1kg_amr_hg19': 'https://www.dropbox.com/scl/fi/bxa4zfngsxsc38rhtiv8c/AMR.ALL.split_norm_af.1kgp3v5.hg19.vcf.gz?rlkey=ibcn8hb1n8n36j3u0jfzci267&dl=1',\n",
    "#  '1kg_amr_hg19_md5': '68d3cdf01cbabdae6e74a07795fa881c',\n",
    "#  '1kg_amr_hg19_tbi': 'https://www.dropbox.com/scl/fi/1zk16x7h4r89jurzwu05u/AMR.ALL.split_norm_af.1kgp3v5.hg19.vcf.gz.tbi?rlkey=b4cere4w38zvzyfitfge3r8n0&dl=1',\n",
    "#  '1kg_sas_hg38': 'https://www.dropbox.com/scl/fi/jr3l5zz42py3kny2bccmj/SAS.ALL.split_norm_af.1kg_30x.hg38.vcf.gz?rlkey=x0t6tsy71jxzf021wfqdn8k5q&dl=1',\n",
    "#  '1kg_sas_hg38_md5': 'e5d79bea1958aa50c23f618d342ccc83',\n",
    "#  '1kg_sas_hg38_tbi': 'https://www.dropbox.com/scl/fi/02oia4ur5r7w9qgiuf6i9/SAS.ALL.split_norm_af.1kg_30x.hg38.vcf.gz.tbi?rlkey=00p9rxe0xzfs6hr1rg4d8oadm&dl=1',\n",
    "#  '1kg_amr_hg38': 'https://www.dropbox.com/scl/fi/4t4tyuhzp78uyb6tgkroq/AMR.ALL.split_norm_af.1kg_30x.hg38.vcf.gz?rlkey=p96gbs1tcdia31jnjv1b82kuz&dl=1',\n",
    "#  '1kg_amr_hg38_md5': '229fbd610001cf6f137b7f738352a44a',\n",
    "#  '1kg_amr_hg38_tbi': 'https://www.dropbox.com/scl/fi/x0dby543tr9xpaqj2i0ba/AMR.ALL.split_norm_af.1kg_30x.hg38.vcf.gz.tbi?rlkey=uj8o7j0cy0spipe174jn54sqs&dl=1',\n",
    "#  '1kg_afr_hg19': https://www.dropbox.com/scl/fi/tq4w9lyt5z47ym7grtrxg/AFR.ALL.split_norm_af.1kgp3v5.hg19.vcf.gz?rlkey=k3bimeu3yr5loq8hohba5mr6k&dl=1,\n",
    "#  '1kg_afr_hg19_md5': 'f7b4425f39e8292dce6f13711e7f6c50',\n",
    "#  '1kg_afr_hg19_tbi': 'https://www.dropbox.com/scl/fi/0giiptu0btwj1kfm6jdzr/AFR.ALL.split_norm_af.1kgp3v5.hg19.vcf.gz.tbi?rlkey=ucb5weprsc5prcg8hvtgmruxx&dl=1',\n",
    "#  '1kg_pan_hg19': 'https://www.dropbox.com/scl/fi/6b4j9z9knmllfnbx86aw6/PAN.ALL.split_norm_af.1kgp3v5.hg19.vcf.gz?rlkey=eento8vg06zyrkvooc9wd4cvu&dl=1',\n",
    "#  '1kg_pan_hg19_md5': 'fed846482204487b60d33b21ddb18106',\n",
    "#  '1kg_pan_hg19_tbi': 'https://www.dropbox.com/scl/fi/stco946scio5tvto0ln4j/PAN.ALL.split_norm_af.1kgp3v5.hg19.vcf.gz.tbi?rlkey=hfh53beb627lmqwv3d8mzqy0c&dl=1',\n",
    "#  '1kg_afr_hg38': 'https://www.dropbox.com/scl/fi/239xmm7qijtnsks97chc9/AFR.ALL.split_norm_af.1kg_30x.hg38.vcf.gz?rlkey=47en5fk1icbekpg7we3uot9g8&dl=1',\n",
    "#  '1kg_afr_hg38_md5': '3bb7923be0809a324d7b7633b8d58a3b',\n",
    "#  '1kg_afr_hg38_tbi': 'https://www.dropbox.com/scl/fi/3y3pg4yqwo2jaaamx1c8f/AFR.ALL.split_norm_af.1kg_30x.hg38.vcf.gz.tbi?rlkey=say0ihfwa51z3otgn4bjtze8p&dl=1',\n",
    "#  '1kg_pan_hg38': 'https://www.dropbox.com/scl/fi/nf01487smtmeq243ihfwm/PAN.ALL.split_norm_af.1kg_30x.hg38.vcf.gz?rlkey=3pefbkzxwcnejx4inynifpft7&dl=1',\n",
    "#  '1kg_pan_hg38_md5': '23bb86d748c4a66e85e087f647e8b60e',\n",
    "#  '1kg_pan_hg38_tbi': 'https://www.dropbox.com/scl/fi/hu7cttr4cenw5yjsm2775/PAN.ALL.split_norm_af.1kg_30x.hg38.vcf.gz.tbi?rlkey=568u7bkvkybm4wt2q9284o198&dl=1',\n",
    "#  'dbsnp_v151_hg19': 'https://ftp.ncbi.nih.gov/snp/organisms/human_9606_b151_GRCh37p13/VCF/00-All.vcf.gz',\n",
    "#  'dbsnp_v151_hg19_tbi': 'https://ftp.ncbi.nih.gov/snp/organisms/human_9606_b151_GRCh37p13/VCF/00-All.vcf.gz.tbi',\n",
    "#  'dbsnp_v151_hg38': 'https://ftp.ncbi.nih.gov/snp/organisms/human_9606_b151_GRCh38p7/VCF/00-All.vcf.gz',\n",
    "#  'dbsnp_v151_hg38_tbi': 'https://ftp.ncbi.nih.gov/snp/organisms/human_9606_b151_GRCh38p7/VCF/00-All.vcf.gz.tbi',\n",
    "#  'dbsnp_v156_hg19': 'https://ftp.ncbi.nih.gov/snp/archive/b156/VCF/GCF_000001405.25.gz',\n",
    "#  'dbsnp_v156_hg19_tbi': 'https://ftp.ncbi.nih.gov/snp/archive/b156/VCF/GCF_000001405.25.gz.tbi',\n",
    "#  'dbsnp_v156_hg38': 'https://ftp.ncbi.nih.gov/snp/archive/b156/VCF/GCF_000001405.40.gz',\n",
    "#  'dbsnp_v156_hg38_tbi': 'https://ftp.ncbi.nih.gov/snp/archive/b156/VCF/GCF_000001405.40.gz.tbi',\n",
    "#  'ucsc_genome_hg19': 'http://hgdownload.cse.ucsc.edu/goldenpath/hg19/bigZips/hg19.fa.gz',\n",
    "#  'ucsc_genome_hg38': 'https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz',\n",
    "#  '1kg_dbsnp151_hg19_auto': 'https://www.dropbox.com/s/37p2u1xwmux4gwo/1kg_dbsnp151_hg19_auto.txt.gz?dl=1',\n",
    "#  '1kg_dbsnp151_hg19_auto_md5': '7d1e7624fb6e4df7a2f6f05558d436b4',\n",
    "#  '1kg_dbsnp151_hg38_auto': 'https://www.dropbox.com/s/ouf60n7gdz6cm0g/1kg_dbsnp151_hg38_auto.txt.gz?dl=1',\n",
    "#  '1kg_dbsnp151_hg38_auto_md5': '4c7ef2d2415c18c286219e970fdda972',\n",
    "#  'recombination_hg19': 'https://www.dropbox.com/s/wbesl8haxknonuc/recombination_hg19.tar.gz?dl=1',\n",
    "#  'recombination_hg38': 'https://www.dropbox.com/s/vuo8mvqx0fpibzj/recombination_hg38.tar.gz?dl=1',\n",
    "#  'ensembl_hg19_gtf': 'https://ftp.ensembl.org/pub/grch37/current/gtf/homo_sapiens/Homo_sapiens.GRCh37.87.chr.gtf.gz',\n",
    "#  'ensembl_hg38_gtf': 'https://ftp.ensembl.org/pub/release-109/gtf/homo_sapiens//Homo_sapiens.GRCh38.109.chr.gtf.gz',\n",
    "#  'refseq_hg19_gtf': 'https://ftp.ncbi.nlm.nih.gov/refseq/H_sapiens/annotation/GRCh37_latest/refseq_identifiers/GRCh37_latest_genomic.gtf.gz',\n",
    "#  'refseq_hg38_gtf': 'https://ftp.ncbi.nlm.nih.gov/refseq/H_sapiens/annotation/GRCh38_latest/refseq_identifiers/GRCh38_latest_genomic.gtf.gz',\n",
    "#  'testlink': 'https://www.dropbox.com/s/8u7capwge0ihshu/EAS.chr22.split_norm_af.1kgp3v5.vcf.gz?dl=1',\n",
    "#  'testlink_tbi': 'https://www.dropbox.com/s/hdneg53t6u1j6ib/EAS.chr22.split_norm_af.1kgp3v5.vcf.gz.tbi?dl=1'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # download ref SNPID-rsID table first\n",
    "# # hg19 is the same as GRCh37, which is the same as b37, which is the same as 19\n",
    "# # USCSC Genome Browser hg19, SNP information\n",
    "# gl.download_ref(\"ucsc_genome_hg19\")\n",
    "# # combined 1KG and dbSNP151, hg19, autosomes\n",
    "# gl.download_ref(\"1kg_dbsnp151_hg19_auto\")\n",
    "# # gl.download_ref(\"refseq_hg19_gtf\") # gene annotation, hg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gl.download_ref(\"dbsnp_v156_hg19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gl.download_ref(\"dbsnp_v156_hg19_tbi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_ref\n",
    "# gl.download_ref(REF_1KG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_ref(\"recombination_hg19\") - get recombination map for hg19\n",
    "# gl.download_ref(\"recombination_hg19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_ref(\"ensembl_hg19_gtf\") - get the Ensembl GTF file for hg19\n",
    "# gl.download_ref(\"ensembl_hg19_gtf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download_ref(\"refseq_hg19_gtf\") - get the refseq hg19 gtf file\n",
    "# gl.download_ref(\"refseq_hg19_gtf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic check, harmonization, normalization, quality control, and filtering\n",
    "\n",
    "Here we fix the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic check\n",
    "\n",
    "Here we apply basic check to the data, which makes sure that:\n",
    "- SNPIDs are of the form chr:bp\n",
    "- orders the data\n",
    "- all alleles are capitalized\n",
    "- does sanity checks on data\n",
    "\n",
    "However, no data is filtered, and normalization (to a reference) is also not applied!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute `bacis_check` function - first we just make sure the data has the expected format, columns, and datatypes.\n",
    "# full data\n",
    "cd_norm_data_sumstats.basic_check(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the form of the data as it is AFTER `basic_check` function\n",
    "# full data\n",
    "cd_norm_data_sumstats.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data_sumstats.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data_sumstats.lookup_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicate and multi-allelic variants\n",
    "\n",
    "Here we remove duplicate and multi-allelic variants, and keep the variants with the lowest P-value for association."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing multiallelic and duplicate variants\n",
    "# mode=d ,remove duplicate.\n",
    "#     remove duplicate SNPs based on 1. SNPID,\n",
    "#     remove duplicate SNPs based on 2. CHR, POS, EA, and NEA\n",
    "#     remove duplicate SNPs based on 3. rsID\n",
    "# mode=m, remove multiallelic variants.\n",
    "#     remove multiallelic SNPs based on 4. CHR, POS\n",
    "# remove=True : remove NAs\n",
    "# keep_col : use which column to sort the values (keep_ascend=True: ascending order)\n",
    "# keep: keep 'first' or 'last'.\n",
    "\n",
    "# gwas_combo_sumstats_sample.remove_dup(\n",
    "#     mode=\"md\",  # remove multi-allelic and duplicate variants\n",
    "#     remove=True,  # remove NAs\n",
    "#     keep_col=\"P\",\n",
    "#     keep_ascend=True,\n",
    "#     keep=\"first\",  # keep the first variant, with the lowest p-value (sorted by that column)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # full dataset\n",
    "cd_norm_data_sumstats.remove_dup(\n",
    "    mode=\"md\",  # remove multi-allelic and duplicate variants\n",
    "    remove=False,  # remove NAs\n",
    "    keep_col=\"P\",\n",
    "    keep_ascend=True,\n",
    "    # keep the first variant, with the lowest p-value (sorted by that column)\n",
    "    keep=\"first\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Harmonization\n",
    "\n",
    "Here we harmonize the data with the reference:\n",
    "- we make sure alleles are oriented according to the reference\n",
    "- we assign rsIDs\n",
    "- we flip alleles (and effect sizes), when necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Align NEA with REF in the reference genome\n",
    "\n",
    "We check if the non-effect allele is aligned with the reference sequence (`hg19`). The status code will be changed accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # .check_ref(): Check if NEA is aligned with the reference sequence. After checking, the tracking status code will be changed accordingly.\n",
    "\n",
    "# # full dataset\n",
    "cd_norm_data_sumstats.check_ref(\n",
    "    ref_seq=gl.get_path(\"ucsc_genome_hg38\"), # ucsc_genome_hg38 - ucsc_genome_hg19\n",
    "    #   chr_dict=gl.get_number_to_NC(build=\"19\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we make sure to flipp the alleles based on the status code\n",
    "\n",
    "# full dataset\n",
    "cd_norm_data_sumstats.flip_allele_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data_sumstats.lookup_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check palindromic SNPs or indistinguishable Indels\n",
    "\n",
    "Here we check for palindromic variants and indistinguishable INDELs and remove these. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .infer_strand() - skipped as this is done\n",
    "# ref_infer= gl.get_path(\"1kg_AFR_hg19\"), # reference vcf file\n",
    "# ref_alt_freq=None,\n",
    "# maf_threshold=0.40, # maf threshold for strand inference\n",
    "# remove_snp=\"\", # remove snps from the data\n",
    "# mode=\"pi\",\n",
    "# n_cores=1,\n",
    "# remove_indel=\"\"\n",
    "\n",
    "# first we check the strand of the data and assign the status code\n",
    "# gwas_combo_sumstats_sample.infer_strand(ref_infer=gl.get_path(\"1kg_AFR_hg19\"), n_cores=6)\n",
    "\n",
    "# full data\n",
    "# gwas_combo_sumstats.infer_strand(ref_infer = gl.get_path(\"1kg_AFR_hg19\"),\n",
    "#                                        n_cores=6)\n",
    "# then we flip the alleles according to the status code\n",
    "# gwas_combo_sumstats_sample.flip_allele_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer the strand of the variants in the summary statistics data\n",
    "# This function checks the strand of the data and assigns the status code accordingly\n",
    "# The reference VCF file and the reference allele frequency column are specified\n",
    "# The number of cores to use for parallel processing is also specified\n",
    "\n",
    "# Define the reference VCF file path and allele frequency column\n",
    "ref_vcf_path = gl.get_path(REF_1KG)  # Use the reference genome specified earlier\n",
    "ref_alt_freq_col = \"AF\"  # Column name for allele frequency in the reference VCF\n",
    "\n",
    "# Infer the strand of the variants\n",
    "cd_norm_data_sumstats.infer_strand(ref_infer=ref_vcf_path, ref_alt_freq=ref_alt_freq_col, n_cores=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data_sumstats.flip_allele_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Assign rsID to the data\n",
    "\n",
    "For the sample dataset this will take not much time. For a dataset with 8M variants it will take about 50-60 minutes! We should fix the SNPIDs such that it includes the alleles. This way the assigning of rsID will go smoothly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rsID annotation based on chr, pos, ref, alt using a VCF file when the SNPID is of the form chr:pos.\n",
    "# .assign_rsid() options\tDataType\tDescription\tDefault\n",
    "# ref_rsid_tsv\tstring\ttsv file path for annotation of commonly used variants using SNPID (like 1:725932:G:A) as key.\t-\n",
    "# ref_rsid_vcf\tstring\tvcf file path for annotation of other variants. .tbi file is also needed.\t-\n",
    "# chr_dict\tdict\ta dictionary for converting 1-25 to CHR in the vcf files.\n",
    "# For example, the notation in dbSNP vcf file is based on RefSeq (like NC_000001.10).\n",
    "# gwaslab provides built-in conversion dictionaries.\n",
    "# gl.get_number_to_NC(build=\"19\")\n",
    "# n_cores\tint\tnumber of cores to use.\n",
    "\n",
    "# gwas_combo_sumstats_sample.assign_rsid(\n",
    "#     n_cores=4,\n",
    "#     # ref_rsid_tsv = gl.get_path(\"1kg_dbsnp151_hg19_auto\"),\n",
    "#     ref_rsid_vcf=\"/Users/slaan3/PLINK/references/dbSNP/GCF_000001405.25.vcf.gz\",  # this works when SNPID is in the format chr:pos\n",
    "#     chr_dict=gl.get_number_to_NC(\n",
    "#         build=\"19\"\n",
    "#     ),  # this is needed as in the VCF file, the chromosome is in NC format\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data_sumstats.assign_rsid(\n",
    "    n_cores=4,\n",
    "    ref_rsid_tsv = gl.get_path(\"1kg_dbsnp151_hg38_auto\"),\n",
    "    #CHANGE!\n",
    "    # ref_rsid_vcf=\"/tpeters/references/GCF_000001405.25.gz\",  # this works when SNPID is in the format chr:pos\n",
    "    chr_dict=gl.get_number_to_NC(\n",
    "        build=\"38\" # was 19\n",
    "    ),  # this is needed as in the VCF file, the chromosome is in NC format\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data_sumstats.fix_id(\n",
    "    fixid=True,\n",
    "    forcefixid=True,\n",
    "    overwrite=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data_sumstats.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd_norm_data_sumstats.assign_rsid(\n",
    "#     n_cores=8,\n",
    "#     # this works for common variants\n",
    "#     ref_rsid_tsv=gl.get_path(\"1kg_dbsnp151_hg19_auto\"),\n",
    "#     # ref_rsid_vcf=gl.get_path(\"dbsnp_v156_hg19\"),\n",
    "#     chr_dict=gl.get_number_to_NC(\n",
    "#         build=\"19\"\n",
    "#     ),  # this is needed as in the VCF file, the chromosome is in NC format\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data_sumstats.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data_sumstats.lookup_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plot frequencies of data against the reference\n",
    "\n",
    "We would like to plot the frequency of the GWAS data compared to the reference data. We do this on a random subset of the data (n=200,000) as this is more efficient and should be representative of the full datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd_norm_data_sumstats_sample = cd_norm_data_sumstats_qc.random_variants(n=400000)\n",
    "# cd_norm_data_sumstats_sample = cd_norm_data_sumstats.random_variants(n=400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd_norm_data_sumstats_sample.check_af(\n",
    "#      ref_infer=gl.get_path(REF_1KG),\n",
    "#     ref_alt_freq=\"AF\",\n",
    "#     n_cores=8,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd_norm_data_sumstats_sample.plot_daf(threshold=0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check allele frequencies against the reference genome\n",
    "# This function compares the allele frequencies in the summary statistics data with those in the reference genome\n",
    "# The reference VCF file and the reference allele frequency column are specified\n",
    "# The number of cores to use for parallel processing is also specified\n",
    "\n",
    "# Define the reference VCF file path and allele frequency column\n",
    "ref_vcf_path = gl.get_path(REF_1KG)  # Use the reference genome specified earlier\n",
    "ref_alt_freq_col = \"AF\"  # Column name for allele frequency in the reference VCF\n",
    "\n",
    "# Check allele frequencies\n",
    "cd_norm_data_sumstats.check_af(\n",
    "    ref_infer=ref_vcf_path,  # Path to the reference VCF file\n",
    "    ref_alt_freq=ref_alt_freq_col,  # Column name for allele frequency in the reference VCF\n",
    "    n_cores=8,  # Number of cores to use for parallel processing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data_sumstats.plot_daf(threshold=0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd_norm_data_sumstats_qc.plot_daf(threshold=0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del cd_norm_data_sumstats_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary\n",
    "\n",
    "So in summary we have done:\n",
    "- a basic check, making sure the chr, pos, alleles are all in order\n",
    "- multi-allelic, weird chromosomes, and duplicate variants are removed\n",
    "- data are harmonized, such that the rsID is added, the alleles are relative to the references (and flipped if needed) \n",
    "\n",
    "> Note, while we do calculate the difference in allele frequencies between the datasets and the reference, we **do not** filter on this for the purpose of sharing through _GWAS Catalog_. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"The parsed and harmonized SumStats object:\\n\")\n",
    "# \n",
    "# cd_norm_data_sumstats.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  cd_norm_data_sumstats.data[\"CAVEAT\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = cd_norm_data_sumstats.data[\"CAVEAT\"].value_counts()\n",
    "\n",
    "# temp.to_csv(\n",
    "#     os.path.join(\n",
    "#         GWASCatalog_loc + \"/META_\" + PHENOTYPE + \"_\" + POPULATION + \".counts_caveats.csv\",\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save or open data\n",
    "Let's save (or open) the summary statistics object for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gwaslab as gl\n",
    "\n",
    "gl.dump_pickle(\n",
    "    cd_norm_data_sumstats,\n",
    "    os.path.join(\n",
    "        GWASCatalog_loc + \"/META_\" + PHENOTYPE + \"_\" + POPULATION + \".b38.gwaslab.pkl\",\n",
    "    ),\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for future reference to open the pickle file\n",
    "\n",
    "cd_norm_data_sumstats = gl.load_pickle(\n",
    "    os.path.join(\n",
    "        GWASCatalog_loc + \"/META_\" + PHENOTYPE + \"_\" + POPULATION + \".b38.gwaslab.pkl\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUMA_loc = os.path.join(GWASCatalog_loc+\"/FUMA/\")\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(FUMA_loc):\n",
    "    # If it doesn't exist, create it\n",
    "    os.makedirs(FUMA_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the output location for the text file\n",
    "output_file = os.path.join(\n",
    "    FUMA_loc + \"/META_\" + PHENOTYPE + \"_\" + POPULATION + \".b38.gwaslab.txt\"\n",
    ")\n",
    "\n",
    "# Use the write method to export the sumstat data to a text file \n",
    "cd_norm_data_sumstats.to_format(output_file, fmt=\"fastgwa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gwaslab as gl\n",
    "\n",
    "cd_norm_data_sumstats.log.show()\n",
    "\n",
    "cd_norm_data_sumstats.log.save(\n",
    "    os.path.join(\n",
    "        GWASCatalog_loc + \"/META_\" + PHENOTYPE + \"_\" + POPULATION + \".b38.gwaslab.log\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "LOAD PICKLE FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd_norm_data_sumstats_qc = gl.load_pickle(\n",
    "#     os.path.join(\n",
    "#         GWASCatalog_loc + \"/META_\" + PHENOTYPE + \"_\" + POPULATION + \".b37.gwaslab.qc.pkl\",\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Parquet\n",
    "\n",
    "Save it as a parquet for easy loading as a dataframe in other programs (_e.g._ `R`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pyarrow as pa\n",
    "# import pyarrow.parquet as pq\n",
    "\n",
    "# # Convert DataFrame to Apache Arrow Table\n",
    "# temp_table = pa.Table.from_pandas(cd_norm_data_sumstats.data)\n",
    "\n",
    "# # Parquet with Brotli compression\n",
    "# pq.write_table(\n",
    "#     temp_table,\n",
    "#     os.path.join(\n",
    "#         GWASCatalog_loc + \"/META_\" + PHENOTYPE + \"_\" + POPULATION + \".b37.gwaslab.parquet\",\n",
    "#     ),\n",
    "#     compression=\"BROTLI\",\n",
    "# )\n",
    "# # we delete the temporary table object\n",
    "# del temp_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GWAS Catalog\n",
    "\n",
    "Save it in `GWAS catalog`-format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd_norm_data_sumstats.to_format(\n",
    "#     os.path.join(GWASCatalog_loc + \"/META_\" + PHENOTYPE + \"_\" + POPULATION + \".b37.gwaslab\"),\n",
    "#     fmt=\"ssf\",\n",
    "#     build=\"19\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we need to validate results for `GWAS Catalog`. For this purpose we created an environment based on python 3.9 and used `gwas-sumstats-tools` (https://github.com/EBISPOT/gwas-sumstats-tools). We remove any remaining variant with alleles formatted such that is unacceptable for `GWAS Catalog`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter based on the following:\n",
    "\n",
    "- CAVEAT == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd_norm_data_sumstats.data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd_norm_data_sumstats_qc = cd_norm_data_sumstats.filter_value(\n",
    "#     '(EAF>=0.01 & EAF<0.99 & DF>=1)'\n",
    "# )\n",
    "cd_norm_data_sumstats_qc = cd_norm_data_sumstats.filter_value(\n",
    "    '(DAF>=-0.12 & DAF<0.12)'\n",
    ")\n",
    "\n",
    "# cd_norm_data_sumstats_qc = cd_norm_data_sumstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data_sumstats_qc.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data_sumstats_qc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data_sumstats_qc.lookup_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gwaslab as gl\n",
    "\n",
    "gl.dump_pickle(\n",
    "    cd_norm_data_sumstats_qc,\n",
    "    os.path.join(\n",
    "        GWASCatalog_loc + \"/META_\" + PHENOTYPE + \"_\" + POPULATION + \".b38.gwaslab.qc.001.pkl\",\n",
    "    ),\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd_norm_data_sumstats_qc.to_format(\n",
    "#     os.path.join(GWASCatalog_loc + \"/META_\" + PHENOTYPE + \"_\" + POPULATION + \".b37.qc.001.gwaslab\"),\n",
    "#     fmt=\"ssf\",\n",
    "#     build=\"19\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd_norm_data_sumstats_qc = gl.load_pickle(\n",
    "#     os.path.join(\n",
    "#         GWASCatalog_loc + \"/META_\" + PHENOTYPE + \"_\" + POPULATION + \".b38.gwaslab.qc.001.pkl\",\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd_norm_data_sumstats_qc = gl.load_pickle(\n",
    "#     os.path.join(\n",
    "#         GWASCatalog_loc + \"/META_\" + PHENOTYPE + \"_\" + POPULATION + \".b38.2.gwaslab.qc.001.pkl\",\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisations\n",
    "\n",
    "Here we attempt to create a genome-wide plot annotating significant genes.\n",
    "\n",
    "#### Manhattan and stratified QQ plots\n",
    "\n",
    "Here we create an automatically annotated Manhattan plot. Annotation is based on hg19 (GRCh37), ensembl 87 (https://ftp.ensembl.org/pub/grch37/release-109/gtf/homo_sapiens/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # manhattan and qq plot\n",
    "cd_norm_data_sumstats.plot_mqq(\n",
    "    skip=2,\n",
    "    cut=10,\n",
    "    sig_line=True,\n",
    "    sig_level=5e-8,\n",
    "    anno=\"GENENAME\",\n",
    "    anno_style=\"right\",\n",
    "    windowsizekb=500,\n",
    "    arm_offset=2,\n",
    "    repel_force=0.02,  # default 0.01\n",
    "    use_rank=True,\n",
    "    build=\"38\",\n",
    "    # mode=\"m\",\n",
    "    stratified=True,\n",
    "    drop_chr_start=True,\n",
    "    figargs={\"figsize\": (20, 8), \"dpi\": 300},\n",
    "    title=\"\" + PHENOTYPE + \"_\" + POPULATION + \"\",\n",
    "    save=os.path.join(\n",
    "        PLOTS_loc, \"manhattan.500kb.300dpi.\" + PHENOTYPE + \"_\" + POPULATION + \".pdf\"),\n",
    "    saveargs={\"dpi\": 300},\n",
    "    verbose=True,\n",
    "    marker_size=(25,25)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manhattan and qq plot\n",
    "cd_norm_data_sumstats_qc.plot_mqq(\n",
    "    skip=2,\n",
    "    cut=10,\n",
    "    sig_line=True,\n",
    "    sig_level=5e-8,\n",
    "    anno=\"GENENAME\",\n",
    "    anno_style=\"right\",\n",
    "    windowsizekb=500,\n",
    "    arm_offset=2,\n",
    "    repel_force=0.02,  # default 0.01\n",
    "    use_rank=True,\n",
    "    build=\"38\",\n",
    "    # mode=\"m\",\n",
    "    stratified=True,\n",
    "    drop_chr_start=True,\n",
    "    figargs={\"figsize\": (20, 8), \"dpi\": 300},\n",
    "   title=\"\" + PHENOTYPE + \"_\" + POPULATION + \"_\" + \"QC\" + \"\",\n",
    "    save=os.path.join(\n",
    "        PLOTS_loc, \"manhattan.500kb.300dpi.\" + PHENOTYPE + \"_\" + POPULATION + \".QC.pdf\"),\n",
    "    saveargs={\"dpi\": 300},\n",
    "    verbose=True,\n",
    "    marker_size=(25,25)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top loci\n",
    "\n",
    "We inventory the top loci. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data_sumstats.get_lead(anno=True, sig_level=5e-8, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_qc_loci = cd_norm_data_sumstats_qc.get_lead(anno=True, sig_level=5e-6, verbose=True)\n",
    "top_qc_loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in top_qc_loci.iterrows():\n",
    "    print(f\"Regional plot for gene {row['GENE']}\")\n",
    "    min_pos = row[\"POS\"] - 1000000\n",
    "    max_pos = row[\"POS\"] + 1000000\n",
    "    if min_pos < 0:\n",
    "        min_pos = 0\n",
    "    cd_norm_data_sumstats_qc.plot_mqq(mode=\"r\",build=\"38\", region=(row[\"CHR\"],min_pos,max_pos),region_grid=True, figargs={\"figsize\": (25, 15), \"dpi\": 300},\n",
    "                                      vcf_path=gl.get_path(\"1kg_eur_hg38\"),\n",
    "                                      title= row[\"GENE\"] + \"_\" + PHENOTYPE + \"_\" + POPULATION + \"_\" + \"QC\",\n",
    "                                      save=os.path.join(REG_PLOTS_loc, \"regional.plot.chr\" + str(row[\"CHR\"]) + \".pos\" + str(row[\"POS\"]) + \".\" + row[\"GENE\"] + \".300dpi.\" + PHENOTYPE + \"_\" + POPULATION + \".QC.pdf\"), \n",
    "                                      saveargs={\"dpi\": 300},)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cd_norm_data_sumstats_qc, cd_norm_data_sumstats_qc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOAD not-normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data_sumstats_qc = gl.load_pickle(\n",
    "    os.path.join(\n",
    "        GWASCatalog_loc + \"/META_\" + PHENOTYPE + \"_\" + POPULATION + \".b38.gwaslab.qc.001.pkl\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_norm_data_sumstats_qc2 = gl.load_pickle(\"/Users/tpeters4/Library/CloudStorage/OneDrive-UMCUtrecht/Documenten/PhD Documents/Projecten/GWAS/Post_regenie_pgen/GWASCatalog/META_IPH_CLAM_EUR.b38.gwaslab.qc.001.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the output location for the text file\n",
    "output_file = os.path.join(\n",
    "    GWASCatalog_loc + \"/GWAS_SSF/META_\" + PHENOTYPE + \"_\" + POPULATION + \".b38.gwaslab\"\n",
    ")\n",
    "\n",
    "# Use the write method to export the sumstat data to a text file \n",
    "cd_norm_data_sumstats_qc.to_format(output_file, fmt=\"ssf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the output location for the text file\n",
    "output_file = os.path.join(\n",
    "    GWASCatalog_loc + \"/GWAS_SSF/META_IPH_CLAM_\" + POPULATION + \".b38.gwaslab\"\n",
    ")\n",
    "\n",
    "# Use the write method to export the sumstat data to a text file \n",
    "cd_norm_data_sumstats_qc2.to_format(output_file, fmt=\"ssf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lead SNPs from the summary statistics data\n",
    "# This function identifies the lead SNPs that are genome-wide significant\n",
    "# anno=True adds annotation to the lead SNPs\n",
    "# sig_level=5e-5 sets the significance threshold\n",
    "# verbose=True enables verbose output\n",
    "top_qc_loci = cd_norm_data_sumstats_qc.get_lead(anno=True, sig_level=5e-5, verbose=True)\n",
    "\n",
    "# Display the top loci\n",
    "top_qc_loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the lead SNPs from the second summary statistics data\n",
    "# This function identifies the lead SNPs that are genome-wide significant\n",
    "# anno=True adds annotation to the lead SNPs\n",
    "# sig_level=5e-5 sets the significance threshold\n",
    "# verbose=True enables verbose output\n",
    "top_qc_loci2 = cd_norm_data_sumstats_qc2.get_lead(anno=True, sig_level=5e-5, verbose=True)\n",
    "\n",
    "# Display the top loci\n",
    "top_qc_loci2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the sets of rsIDs from the top loci of both datasets\n",
    "set1 = set(top_qc_loci[\"rsID\"])\n",
    "set2 = set(top_qc_loci2[\"rsID\"])\n",
    "\n",
    "# Print the sets of rsIDs\n",
    "print(\"Set of rsIDs from the first dataset:\", set1)\n",
    "print(\"Set of rsIDs from the second dataset:\", set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_genes = [\"CCXL1\", \"CCXL3\", \"F5\", \"HMOX1\", \"CCR1\", \"CCR2\", \"MMP9\", \"FOS\", \"TIMP3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_gene_coordinates(gene_symbols):\n",
    "    \"\"\"\n",
    "    Fetches the genomic coordinates for a list of gene symbols using the Ensembl REST API.\n",
    "\n",
    "    Parameters:\n",
    "    gene_symbols (list): List of gene symbols to fetch coordinates for.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary where keys are gene symbols and values are dictionaries with chromosome, start, and end positions.\n",
    "    \"\"\"\n",
    "    gene_to_coords = {}\n",
    "    url_template = \"https://rest.ensembl.org/lookup/symbol/homo_sapiens/{}?content-type=application/json\"\n",
    "    \n",
    "    for gene in gene_symbols:\n",
    "        response = requests.get(url_template.format(gene))\n",
    "        if response.ok:\n",
    "            data = response.json()\n",
    "            gene_to_coords[gene] = {\n",
    "                \"chrom\": data[\"seq_region_name\"],\n",
    "                \"start\": data[\"start\"],\n",
    "                \"end\": data[\"end\"]\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Could not fetch coordinates for gene: {gene}\")\n",
    "    \n",
    "    return gene_to_coords\n",
    "\n",
    "# Example usage\n",
    "lookup_genes = [\"CCXL1\", \"CCXL3\", \"F5\", \"HMOX1\", \"CCR1\", \"CCR2\", \"MMP9\", \"FOS\", \"TIMP3\"]\n",
    "gene_coords = get_gene_coordinates(lookup_genes)\n",
    "print(gene_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_snps = cd_norm_data_sumstats_qc.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the best SNPs for each gene\n",
    "results_best_snp1 = []\n",
    "\n",
    "# Iterate over each gene in the gene coordinates dictionary\n",
    "for gene in gene_coords:\n",
    "    print(gene)\n",
    "    # Extract chromosome, start, and end positions for the current gene\n",
    "    chrom, start, end = gene_coords[gene][\"chrom\"], gene_coords[gene][\"start\"], gene_coords[gene][\"end\"]\n",
    "    print(f'chrom: {chrom} - start: {start} - end: {end}')\n",
    "    \n",
    "    # Filter SNPs that fall within the current gene's chromosomal range\n",
    "    filtered_snps = df1_snps[(df1_snps[\"CHR\"] == int(chrom)) & \n",
    "                             (df1_snps[\"POS\"] >= int(start)) & \n",
    "                             (df1_snps[\"POS\"] <= int(end))]\n",
    "    \n",
    "    # If there are SNPs in the filtered range, find the one with the lowest p-value\n",
    "    if not filtered_snps.empty:\n",
    "        best_snp = filtered_snps.loc[filtered_snps[\"P\"].idxmin()]\n",
    "        # Append the best SNP information to the results list\n",
    "        results_best_snp1.append({\n",
    "            \"gene\": gene, \n",
    "            \"CHR\": chrom, \n",
    "            \"POS\": best_snp[\"POS\"], \n",
    "            \"EA\": best_snp[\"EA\"], \n",
    "            \"NEA\": best_snp[\"NEA\"], \n",
    "            \"SNPID\": best_snp[\"SNPID\"], \n",
    "            \"P\": best_snp[\"P\"], \n",
    "            \"rsID\": best_snp[\"rsID\"]\n",
    "        })\n",
    "\n",
    "# Print the results\n",
    "print(results_best_snp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the set of rsIDs\n",
    "set1 = []\n",
    "\n",
    "# Iterate over each SNP in the results_best_snp1 list\n",
    "for snp in results_best_snp1:\n",
    "    # Append the rsID of the SNP to the set1 list\n",
    "    set1.append(snp[\"rsID\"])\n",
    "\n",
    "# Print the set of rsIDs\n",
    "print(set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_snps = cd_norm_data_sumstats_qc2.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the best SNPs for each gene\n",
    "results_best_snp2 = []\n",
    "\n",
    "# Iterate over each gene in the gene coordinates dictionary\n",
    "for gene in gene_coords:\n",
    "    print(gene)\n",
    "    # Extract chromosome, start, and end positions for the current gene\n",
    "    chrom, start, end = gene_coords[gene][\"chrom\"], gene_coords[gene][\"start\"], gene_coords[gene][\"end\"]\n",
    "    print(f'chrom: {chrom} - start: {start} - end: {end}')\n",
    "    \n",
    "    # Filter SNPs that fall within the current gene's chromosomal range\n",
    "    filtered_snps = df2_snps[(df2_snps[\"CHR\"] == int(chrom)) & \n",
    "                             (df2_snps[\"POS\"] >= int(start)) & \n",
    "                             (df2_snps[\"POS\"] <= int(end))]\n",
    "    \n",
    "    # If there are SNPs in the filtered range, find the one with the lowest p-value\n",
    "    if not filtered_snps.empty:\n",
    "        best_snp = filtered_snps.loc[filtered_snps[\"P\"].idxmin()]\n",
    "        # Append the best SNP information to the results list\n",
    "        results_best_snp2.append({\n",
    "            \"gene\": gene, \n",
    "            \"CHR\": chrom, \n",
    "            \"POS\": best_snp[\"POS\"], \n",
    "            \"EA\": best_snp[\"EA\"], \n",
    "            \"NEA\": best_snp[\"NEA\"], \n",
    "            \"SNPID\": best_snp[\"SNPID\"], \n",
    "            \"P\": best_snp[\"P\"], \n",
    "            \"rsID\": best_snp[\"rsID\"]\n",
    "        })\n",
    "\n",
    "# Convert the results to a DataFrame and print\n",
    "print(results_best_snp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the set of rsIDs\n",
    "set2 = []\n",
    "\n",
    "# Iterate over each SNP in the results_best_snp2 list\n",
    "for snp in results_best_snp2:\n",
    "    # Append the rsID of the SNP to the set2 list\n",
    "    set2.append(snp[\"rsID\"])\n",
    "\n",
    "# Print the set of rsIDs\n",
    "print(set2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_elements = [\"CCXL1\", \"CCXL3\", \"F5\", \"HMOX1\", \"CCR1\", \"CCR2\", \"MMP9\", \"FOS\", \"TIMP3\"]\n",
    "# common_elements = [\"CCXL1\", \"CCXL3\", \"F5\", \"HMOX1\", \"CCR1\", \"CCR2\", \"MMP9\", \"FOS\", \"TIMP3\"]\n",
    "\n",
    "# print(common_elements)\n",
    "# set1 = top_qc_loci[top_qc_loci[\"GENE\"].isin(common_elements)][\"rsID\"].values\n",
    "# set2 = top_qc_loci2[top_qc_loci2[\"GENE\"].isin(common_elements)][\"rsID\"].values\n",
    "\n",
    "# print(set1)\n",
    "# print(set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common genes between the two sets of top loci\n",
    "common_elements = list(set(top_qc_loci[\"GENE\"]).intersection(set(top_qc_loci2[\"GENE\"])))\n",
    "\n",
    "# Print the common genes\n",
    "print(\"Common genes:\", common_elements)\n",
    "\n",
    "# Extract the rsIDs of the SNPs associated with the common genes from the first dataset\n",
    "highlight_1 = top_qc_loci[top_qc_loci[\"GENE\"].isin(common_elements)][\"rsID\"].values\n",
    "\n",
    "# Extract the rsIDs of the SNPs associated with the common genes from the second dataset\n",
    "highlight_2 = top_qc_loci2[top_qc_loci2[\"GENE\"].isin(common_elements)][\"rsID\"].values\n",
    "\n",
    "# Print the rsIDs from both datasets\n",
    "print(\"rsIDs from the first dataset:\", highlight_1)\n",
    "print(\"rsIDs from the second dataset:\", highlight_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIAMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_vs_norm_sumstats = gl.plot_miami2(\n",
    "    path1=cd_norm_data_sumstats_qc,\n",
    "    path2=cd_norm_data_sumstats_qc2,\n",
    "    sig_level=5e-8,\n",
    "    # sig_line_color=\"blue\",\n",
    "    # suggestive_sig_line = True,\n",
    "    # suggestive_sig_line_color=\"blue\",\n",
    "    # additional_line=[5e-6],\n",
    "    # additional_line_color=[\"grey\"],\n",
    "    titles=[PHENOTYPE, \"IPH_Model\"],\n",
    "    # titles_pad=[0.1, 0.1],\n",
    "    # titles_pad_adjusted=[0.1, 0.1],\n",
    "    id1=\"rsID\",\n",
    "    id2=\"rsID\",\n",
    "    windowsizekb=500,\n",
    "    arm_offset=2,\n",
    "    repel_force=0.02,  # default 0.01\n",
    "    build=\"38\",\n",
    "    # anno_set=[(16, 83035073)], #CHANGE! - select significant snps/ top loci\n",
    "    # highlight=[(16, 83035073)], # CHANGE! - significant snps\n",
    "    highlight_windowkb=500,\n",
    "    save=os.path.join(PLOTS_loc, \"miami.500kb.300dpi.no_genes.\" + PHENOTYPE + \"_vs_IPH_Model.pdf\"),\n",
    "    save_args={\"dpi\": 300},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_vs_norm_sumstats = gl.plot_miami2(\n",
    "    path1=cd_norm_data_sumstats_qc,\n",
    "    path2=cd_norm_data_sumstats_qc2,\n",
    "    sig_level=5e-8,\n",
    "    # sig_line_color=\"blue\",\n",
    "    # suggestive_sig_line = True,\n",
    "    # suggestive_sig_line_color=\"blue\",\n",
    "    # additional_line=[5e-8],\n",
    "    # additional_line_color=[\"grey\"],\n",
    "    titles=[PHENOTYPE, \"IPH_Model\"],\n",
    "    # titles_pad=[0.1, 0.1],\n",
    "    # titles_pad_adjusted=[0.1, 0.1],\n",
    "    id1=\"rsID\",\n",
    "    id2=\"rsID\",\n",
    "    anno1=\"GENENAME\",\n",
    "    anno_style1=\"right\",\n",
    "    anno_set1=list(set1),\n",
    "    anno2=\"GENENAME\",\n",
    "    anno_style2=\"right\",\n",
    "    anno_set2=list(set2),\n",
    "    highlight1=list(set1),\n",
    "    highlight2=list(set2),\n",
    "    windowsizekb=500,\n",
    "    arm_offset=2,\n",
    "    repel_force=0.02,  # default 0.01\n",
    "    build=\"38\",\n",
    "    # anno_set=[(16, 83035073)], #CHANGE! - select significant snps/ top loci\n",
    "    # highlight=[(16, 83035073)], # CHANGE! - significant snps\n",
    "    highlight_windowkb=500,\n",
    "    save=os.path.join(PLOTS_loc, \"miami.500kb.300dpi.manual_genes.\" + PHENOTYPE + \"_vs_IPH_Model.pdf\"),\n",
    "    save_args={\"dpi\": 300},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_vs_norm_sumstats = gl.plot_miami2(\n",
    "    path1=cd_norm_data_sumstats_qc,\n",
    "    path2=cd_norm_data_sumstats_qc2,\n",
    "    sig_level=5e-6,\n",
    "    sig_line_color=\"blue\",\n",
    "    # suggestive_sig_line = True,\n",
    "    # suggestive_sig_line_color=\"blue\",\n",
    "    additional_line=[5e-8],\n",
    "    additional_line_color=[\"grey\"],\n",
    "    titles=[PHENOTYPE, \"IPH_Model\"],\n",
    "    # titles_pad=[0.1, 0.1],\n",
    "    # titles_pad_adjusted=[0.1, 0.1],\n",
    "    id1=\"rsID\",\n",
    "    id2=\"rsID\",\n",
    "    anno1=\"GENENAME\",\n",
    "    anno_style1=\"right\",\n",
    "    anno_set1=list(set1),\n",
    "    anno2=\"GENENAME\",\n",
    "    anno_style2=\"right\",\n",
    "    anno_set2=list(set2),\n",
    "    highlight1=higlight_1,\n",
    "    highlight2=higlight_2,\n",
    "    windowsizekb=500,\n",
    "    arm_offset=2,\n",
    "    repel_force=0.02,  # default 0.01\n",
    "    build=\"38\",\n",
    "    # anno_set=[(16, 83035073)], #CHANGE! - select significant snps/ top loci\n",
    "    # highlight=[(16, 83035073)], # CHANGE! - significant snps\n",
    "    highlight_windowkb=500,\n",
    "    save=os.path.join(PLOTS_loc, \"miami.500kb.300dpi.genes.\" + PHENOTYPE + \"_vs_IPH_Model.pdf\"),\n",
    "    save_args={\"dpi\": 300},\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EFFECT SIZE\n",
    "Compare effect size between phenotypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gl.compare_effect(\n",
    "    cd_norm_data_sumstats_qc,\n",
    "    cd_norm_data_sumstats_qc2,\n",
    "    mode=\"beta\",\n",
    "    label=[PHENOTYPE,\"IPH CLAM\",\"Both\",\"None\"],\n",
    "    sig_level=5e-6,\n",
    "    legend_title=r\"$ P < 5 x 10^{-8}$ in:\",\n",
    "    legend_title2=r\"Heterogeneity test:\",\n",
    "    legend_pos=\"upper left\",\n",
    "    #  legend_args=None,\n",
    "    xylabel_prefix=\"Per-allele effect size in \",\n",
    "    is_reg=True,\n",
    "    is_45_helper_line=True,\n",
    "    anno=True,\n",
    "    # anno_min=0,\n",
    "    # anno_min1=0,\n",
    "    # anno_min2=0,\n",
    "    # anno_diff=0,\n",
    "    # is_q=False,\n",
    "    q_level=0.05,\n",
    "    # anno_het=False,\n",
    "    # r_se=False,\n",
    "    #  fdr=False,\n",
    "    #  legend_mode=\"full\",\n",
    "     save=True,\n",
    "    # saveargs=None,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "set1 = top_qc_loci[[\"SNPID\", \"GENE\"]]\n",
    "set2 = top_qc_loci2[[\"SNPID\", \"GENE\"]]\n",
    "\n",
    "# find the union of the sets and converting resultant set to list\n",
    "suggestive_snps = pd.concat([set1, set2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestive_snps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the summary statistics objects to pandas DataFrames\n",
    "df1 = cd_norm_data_sumstats_qc.data\n",
    "df2 = cd_norm_data_sumstats_qc2.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'lenght df1: {len(df1)}')\n",
    "print(f'lenght df2: {len(df2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract beta values and SNP IDs from both datasets\n",
    "beta1_df = cd_norm_data_sumstats_qc.data[[\"rsID\", \"BETA\", \"P\", \"EAF\", \"SNPID\"]].rename(columns={\"BETA\": \"BETA1\", \"P\": \"P1\", \"EAF\": \"EAF1\"})\n",
    "beta2_df = cd_norm_data_sumstats_qc2.data[[\"rsID\", \"BETA\", \"P\", \"EAF\"]].rename(columns={\"BETA\": \"BETA2\", \"P\": \"P2\", \"EAF\": \"EAF2\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_beta1 = beta1_df[beta1_df.duplicated(subset=\"rsID\", keep=False)]\n",
    "duplicates_beta2 = beta2_df[beta2_df.duplicated(subset=\"rsID\", keep=False)]\n",
    "\n",
    "print(f\"Number of duplicate rsID in beta1_df: {len(duplicates_beta1)}\")\n",
    "print(f\"Number of duplicate rsID in beta2_df: {len(duplicates_beta2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1_df = beta1_df[beta1_df['rsID'].notna()]\n",
    "beta1_df = beta1_df.drop_duplicates(subset=[\"rsID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta2_df = beta2_df[beta2_df['rsID'].notna()]\n",
    "beta2_df = beta2_df.drop_duplicates(subset=[\"rsID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'lenght df1: {len(df1)}')\n",
    "print(f'lenght df2: {len(df2)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'lenght df1: {len(beta1_df)}')\n",
    "print(f'lenght df2: {len(beta2_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the beta values on SNP ID to ensure we're correlating the same variants\n",
    "# merged_betas = pd.merge(beta1_df, beta2_df, on=\"rsID\")\n",
    "merged_betas = beta1_df.merge(beta2_df, on=\"rsID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_betas = merged_betas[(merged_betas[\"P1\"] > 0.0) & (merged_betas[\"P2\"] > 0.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_betas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'lenght merged: {len(merged_betas)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestive_merged_betas = suggestive_snps.merge(merged_betas, on=\"SNPID\", how=\"left\")\n",
    "# suggestive_merged_betas = merged_betas[(merged_betas[\"P1\"] < 0.001) & (merged_betas[\"P2\"] < 0.001)]\n",
    "suggestive_merged_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'length merged: {len(suggestive_merged_betas)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Calculate the Pearson correlation between the beta values\n",
    "correlation, p_value = pearsonr(merged_betas[\"BETA1\"], merged_betas[\"BETA2\"])\n",
    "\n",
    "print(f\"Pearson correlation: {correlation}\")\n",
    "print(f\"P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Calculate the Pearson correlation between the beta values\n",
    "correlation, p_value = pearsonr(suggestive_merged_betas[\"BETA1\"], suggestive_merged_betas[\"BETA2\"])\n",
    "\n",
    "print(f\"Pearson correlation: {correlation}\")\n",
    "print(f\"P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_betas.loc[merged_betas['BETA1'] > 0, 'BETA1_bin'] = 1\n",
    "merged_betas.loc[merged_betas['BETA1'] <= 0, 'BETA1_bin'] = -1\n",
    "\n",
    "merged_betas.loc[merged_betas['BETA2'] > 0, 'BETA2_bin'] = 1\n",
    "merged_betas.loc[merged_betas['BETA2'] <= 0, 'BETA2_bin'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard_score(merged_betas[\"BETA1_bin\"], merged_betas[\"BETA2_bin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Calculate the Pearson correlation between the beta values\n",
    "correlation, p_value = pearsonr(merged_betas[\"BETA1_bin\"], merged_betas[\"BETA2_bin\"])\n",
    "\n",
    "print(f\"Pearson correlation: {correlation}\")\n",
    "print(f\"P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_grouped_correlation(df, bin_column, bins=5):\n",
    "    # Create the bins dynamically\n",
    "    df[f'{bin_column}_bins'] = pd.cut(df[bin_column], bins=bins)\n",
    "    \n",
    "    # Apply groupby and calculate both the correlation and the count of values in each bin\n",
    "    results = df.groupby(f'{bin_column}_bins').apply(\n",
    "        lambda x: pd.Series({\n",
    "            'correlation': x[['BETA1', 'BETA2']].corr(method='pearson').iloc[0, 1],\n",
    "            'binned_correlation' : x[['BETA1_bin', 'BETA2_bin']].corr(method='pearson').iloc[0, 1],\n",
    "            'binned_jaccard' : jaccard_score(x[\"BETA1_bin\"], x[\"BETA2_bin\"]),\n",
    "            'count': x.shape[0]\n",
    "        })\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example: Group by P1 with 10 bins and show correlation and count\n",
    "# grouped_corr = calculate_grouped_correlation(merged_betas, 'P1', bins=10)\n",
    "grouped_corr = calculate_grouped_correlation(merged_betas, 'P2', bins=50)\n",
    "print(grouped_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_merged_beta = merged_betas[(merged_betas[\"P1\"] < 0.02) & (merged_betas[\"P2\"] < 0.02)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sign matrix where each value is 1, -1, or 0\n",
    "sign_matrix = np.sign(keep_merged_beta[[\"BETA1\", \"BETA2\"]])\n",
    "\n",
    "# Check if all beta values in each row are pointing in the same direction\n",
    "same_direction = (sign_matrix.nunique(axis=1) == 1)\n",
    "\n",
    "# Add the result as a new column to the DataFrame\n",
    "keep_merged_beta['same_direction'] = same_direction\n",
    "\n",
    "print(keep_merged_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counted = keep_merged_beta[\"same_direction\"].value_counts()\n",
    "counted_true = keep_merged_beta[\"same_direction\"].value_counts().tolist()[0]\n",
    "counted_false = keep_merged_beta[\"same_direction\"].value_counts().tolist()[1]\n",
    "counted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Beta values in same direction: {(counted_true/len(keep_merged_beta)):.2f}%\")\n",
    "print(f\"Beta values in same direction: {(counted_false/len(keep_merged_beta)):.2f}%\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_merged_beta[\"same_direction\"].value_counts().tolist()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix of the beta columns\n",
    "correlation_matrix = keep_merged_beta[[\"BETA1\", \"BETA2\"]].corr()\n",
    "\n",
    "# Extract the upper triangle of the correlation matrix, excluding the diagonal\n",
    "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Calculate the mean of the upper triangle correlations\n",
    "mean_correlation = upper_triangle.stack().mean()\n",
    "\n",
    "print(\"Average pairwise correlation:\", mean_correlation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_merged_beta[[\"BETA1\", \"BETA2\"]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "suggestive_merged_betas = suggestive_merged_betas.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "import pandas as pd\n",
    "from easy_entrez import EntrezAPI\n",
    "import myvariant\n",
    "import json\n",
    "from types import SimpleNamespace\n",
    "\n",
    "data = '{\"name\": \"John Smith\", \"hometown\": {\"name\": \"New York\", \"id\": 123}}'\n",
    "\n",
    "# Your list of RSIDs\n",
    "rsids = suggestive_merged_betas['rsID']  # Example RSIDs\n",
    "\n",
    "# Function to fetch gene symbols from NCBI\n",
    "def fetch_gene_symbol(rsid):\n",
    "\n",
    "    mv = myvariant.MyVariantInfo()\n",
    "    query_out = mv.query(rsid, scopes='dbsnp.rsid', fields='dbsnp')\n",
    "    # print(query_out)\n",
    "    if 'gene' in query_out[\"hits\"][0][\"dbsnp\"]:\n",
    "        if \"symbol\" in query_out[\"hits\"][0][\"dbsnp\"][\"gene\"]:\n",
    "            symbol = query_out[\"hits\"][0][\"dbsnp\"][\"gene\"][\"symbol\"]\n",
    "            # print(symbol)\n",
    "            return symbol\n",
    "        else:\n",
    "            return rsid\n",
    "    else:\n",
    "        return rsid\n",
    "\n",
    "    # entrez_api = EntrezAPI('Project name', 'your@mail.com')\n",
    "    # rs6311 = entrez_api.fetch([rsid], max_results=1, database='snp').data\n",
    "    # namespaces = {'ns0': 'https://www.ncbi.nlm.nih.gov/SNP/docsum'}\n",
    "    # genes = [\n",
    "    #     name.text\n",
    "    #     for name in rs6311.findall('.//ns0:GENE_E/ns0:NAME', namespaces)\n",
    "    # ]\n",
    "    # print(genes)\n",
    "    # return genes\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "# results = pd.DataFrame({'RSID': rsids, 'GeneSymbol': [fetch_gene_symbol(rsid) for rsid in rsids]})\n",
    "\n",
    "# print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "from adjustText import adjust_text  # Import for handling text overlaps\n",
    "\n",
    "# Example dataframe columns\n",
    "BETA1 = suggestive_merged_betas['BETA1']\n",
    "BETA2 = suggestive_merged_betas['BETA2']\n",
    "P1 = suggestive_merged_betas['P1']  # P-values for the first set\n",
    "P2 = suggestive_merged_betas['P2']  # P-values for the second set\n",
    "AnnotationColumn = suggestive_merged_betas['rsID']  # The column used for annotations\n",
    "\n",
    "# For the size of the bubbles, we use the -log10(p-values) to emphasize smaller p-values\n",
    "p_together = -np.log10((P1 + P2) / 2)\n",
    "bubble_size = p_together\n",
    "# Small value to avoid division by zero\n",
    "epsilon = 1e-10\n",
    "# Calculate the closeness\n",
    "p_closeness = 1 - (np.abs(P1 - P2) / (np.maximum(P1, P2) + epsilon))\n",
    "# bubble_size = p_closeness\n",
    "\n",
    "# Normalize bubble size to a reasonable range for plotting\n",
    "bubble_size = 200 * (bubble_size - bubble_size.min()) / (bubble_size.max() - bubble_size.min()) + 20\n",
    "\n",
    "# Define a threshold for high values (adjust as needed)\n",
    "threshold = np.mean(p_together) + 2 * np.std(p_together)\n",
    "# threshold = 0\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(BETA1, BETA2, s=bubble_size, c=p_together, cmap='coolwarm', alpha=0.7, edgecolors=\"w\", linewidth=1)\n",
    "\n",
    "# Collect annotations in a list for adjustText\n",
    "annotations = []\n",
    "\n",
    "# Annotate high values with arrows pointing to the points beneath the text\n",
    "\n",
    "\n",
    "genes= []\n",
    "for i in range(len(p_together)):\n",
    "    if p_together[i] > threshold:\n",
    "        # gene = fetch_gene_symbol(AnnotationColumn[i])\n",
    "        gene = suggestive_merged_betas[suggestive_merged_betas[\"rsID\"] == AnnotationColumn[i]][\"GENE\"].values[0]\n",
    "        print(f\"-log10(p-values): {p_together[i]} -- gene: {gene} -- rsId: {AnnotationColumn[i]}\")\n",
    "        genes.append(gene)\n",
    "        \n",
    "        # Set the annotation position slightly above the point\n",
    "        annotation = plt.annotate(gene, (BETA1[i], BETA2[i]), textcoords=\"offset points\", \n",
    "                                  xytext=(1, 1), ha='center', fontsize=9, \n",
    "                                  arrowprops=dict(arrowstyle=\"->\", lw=0.5, color='black'))  # Arrow points from text to the point\n",
    "        annotations.append(annotation)\n",
    "\n",
    "for i in genes: print(i)\n",
    "\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('BETA value IPH Manual')\n",
    "plt.ylabel('BETA value IPH Model')\n",
    "plt.title(\"BETA's of IPH Manual vs IPH Model with Color by -log10(P-values)\")\n",
    "\n",
    "# Add a color bar to indicate the difference in p-values\n",
    "colorbar = plt.colorbar(scatter, label='-log10((P-value Manual + P-value Model) / 2)')\n",
    "\n",
    "# Adjust the layout to account for the color bar\n",
    "plt.tight_layout()\n",
    "\n",
    "# Automatically adjust the annotations to avoid overlap and align arrows\n",
    "adjust_text(annotations)\n",
    "\n",
    "# Calculate correlation coefficient and p-value\n",
    "correlation_coefficient, p_value = stats.pearsonr(BETA1, BETA2)\n",
    "\n",
    "# Fit a linear regression line to plot\n",
    "slope, intercept = np.polyfit(BETA1, BETA2, 1)\n",
    "regression_line = slope * np.array(BETA1) + intercept\n",
    "\n",
    "# Plot the regression line\n",
    "plt.plot(BETA1, regression_line, color='black', linestyle='--', linewidth=1, \n",
    "         label=f'y = {slope:.2f}x + {intercept:.2f}')\n",
    "\n",
    "# Annotate the plot with the correlation coefficient and p-value\n",
    "plt.text(0.05, 0.95, f'Correlation (r) = {correlation_coefficient:.2f}\\nP-value = {p_value:.2e}', \n",
    "         transform=plt.gca().transAxes, fontsize=10, verticalalignment='top')\n",
    "\n",
    "# Show the plot\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(os.path.join(PLOTS_loc, \"BETA_corr_300dpi.\" + PHENOTYPE + \"_vs_IPH_Model.no_genes.png\"), format='png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_binned_correlation(df, p1_col, p2_col, beta1_col, beta2_col, bins_p1=10, bins_p2=10):\n",
    "    # Bin the P1 and P2 values using the specified number of bins\n",
    "    df['COL1_binned'] = pd.cut(df[p1_col], bins=bins_p1)\n",
    "    df['COL2_binned'] = pd.cut(df[p2_col], bins=bins_p2)\n",
    "    \n",
    "    # Get unique bin labels for sorting later\n",
    "    sorted_p1_bins = df['COL1_binned'].cat.categories\n",
    "    sorted_p2_bins = df['COL2_binned'].cat.categories[::-1]\n",
    "\n",
    "    # Create an empty matrix to hold correlations\n",
    "    correlation_matrix = np.full((len(sorted_p2_bins), len(sorted_p1_bins)), np.nan)\n",
    "\n",
    "    # Fill the matrix with correlations for each pair of binned P1 and P2\n",
    "    for i, p2_bin in enumerate(sorted_p2_bins):\n",
    "        for j, p1_bin in enumerate(sorted_p1_bins):\n",
    "            # Filter data for specific P1 and P2 bins\n",
    "            sub_df = df[(df['COL1_binned'] == p1_bin) & (df['COL2_binned'] == p2_bin)]\n",
    "            \n",
    "            if i ==0 and j ==0:\n",
    "                print(f\"p1 bin: {p1_bin} -- p2 bin: {p2_bin}\")\n",
    "                print(sub_df)\n",
    "            # Check if we have enough values to calculate correlation (minimum 3)\n",
    "            if len(sub_df) >= 3:\n",
    "                correlation = sub_df[[beta1_col, beta2_col]].corr().iloc[0, 1]  # Get correlation\n",
    "                correlation_matrix[i, j] = correlation  # Store the correlation\n",
    "            else:\n",
    "                # Keep it as NaN if there are fewer than 3 values\n",
    "                pass\n",
    "    \n",
    "    # correlation_matrix = np.flip(correlation_matrix,1) # horizontal flip\n",
    "    # sorted_p2_bins = sorted_p2_bins.reverse() # reverse the order of list elementsac\n",
    "\n",
    "    # Plot the heatmap\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    sns.heatmap(correlation_matrix, xticklabels=sorted_p1_bins, yticklabels=sorted_p2_bins, cmap='coolwarm', annot=False, fmt='.1f', linewidth=.5, vmin=-1, vmax=1, cbar_kws={'ticks': [-1, -0.8, -0.6, -0.4, -0.2, 0, 0.2, 0.4, 0.6, 0.8, 1]})\n",
    "    \n",
    "    # Get the current tick positions and labels for the x and y axes\n",
    "    xticks_positions = plt.gca().get_xticks()  # Get current tick positions for the x-axis\n",
    "    yticks_positions = plt.gca().get_yticks()  # Get current tick positions for the y-axis\n",
    "\n",
    "    # Get the current tick labels for the x and y axes\n",
    "    xticks_labels = [item.get_text() for item in plt.gca().get_xticklabels()]\n",
    "    yticks_labels = [item.get_text() for item in plt.gca().get_yticklabels()]\n",
    "\n",
    "    # Replace the first label on the x-axis and y-axis\n",
    "    xticks_labels[0] = '(0.0, 0.02]'\n",
    "    yticks_labels[-1] = '(0.0, 0.02]'\n",
    "\n",
    "    # Set the new tick labels back to the plot\n",
    "    plt.xticks(xticks_positions, xticks_labels)\n",
    "    plt.yticks(yticks_positions, yticks_labels)\n",
    "\n",
    "    plt.xlabel('P values IPH Manual')\n",
    "    plt.ylabel('P values IPH Model')\n",
    "    plt.title(f'Correlation of Beta\\'s between IPH Manual scoring vs IPH Model scoring (Binned by P values)')\n",
    "    plt.savefig(os.path.join(PLOTS_loc,\"BETA_corr_heat_300dpi.\" + PHENOTYPE + \"_vs_IPH_Model.png\"), format='png', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "    return correlation_matrix\n",
    "\n",
    "# Example usage:\n",
    "# Assume your dataframe is 'merged_betas'\n",
    "correlation_matrix = calculate_binned_correlation(\n",
    "    df=merged_betas, \n",
    "    p1_col='P1', \n",
    "    p2_col='P2', \n",
    "    beta1_col='BETA1', \n",
    "    beta2_col='BETA2', \n",
    "    bins_p1=50,  # Number of bins for P1\n",
    "    bins_p2=50   # Number of bins for P2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "postgwas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
